# ML Inference & Model Serving

## Serving Frameworks

**vLLM**: LLM-optimized (PagedAttention, 24x faster inference, Python)
**Triton**: Multi-framework (NVIDIA-backed, dynamic batching, production-grade)
**TorchServe**: PyTorch-native (easy deployment, AWS integration)
**Ray Serve**: General-purpose (autoscaling, model composition, Python)

**Recommendation**: vLLM for LLMs, Triton for multi-framework production.

**Last Updated**: December 2024
